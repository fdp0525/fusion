\documentclass[12pt,twoside]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{listings}

% some definitions for the title page
\newcommand{\reporttitle}{Real-time reconstruction of non-rigid objects from RGBD data}
\newcommand{\reportauthor}{Amelia Gordafarid Crowther}
\newcommand{\supervisor}{Name of supervisor}
\newcommand{\reporttype}{Type of Report/Thesis}
\newcommand{\degreetype}{Computer Science BEng} 

% load some definitions and default packages
\input{includes}

% load title page
\begin{document}
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.
\end{abstract}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
Comment this out if not needed.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

\begin{figure}[tb]
\centering
\includegraphics[width = 0.4\hsize]{./figures/imperial}
\caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
\label{fig:logo}
\end{figure}

Figure~\ref{fig:logo} is an example of a figure. 

\section{Motivation}

The reconstruction of 3-dimensional scenes using RGB-D data has been the subject of a lot of research recently. Due to the wide range of applications in fields such as Robotics and Augmented Reality, it has gained much interest.

However, most existing systems are designed only to reconstruct static geometry. This leads to hard limitations in the adaptability of the system to a changing environment. Worse still, highly deformable or dynamic objects such as a moving person cannot be reconstructed at all by a static system.

There has been a lot of interest in techniques for non-rigid reconstruction, but despite it being a hot research topic, few implementations of proposed systems are widely available.

The aim of this project is to provide such an implementation that can reconstruct a rigid object from a  non-rigid scene in real-time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}

\section{Kinect Camera}

The first and most fundamental element of the system is the sensor used to obtain information about the surface to be reconstructed. The most commonly used sensor for this purpose is the Kinect camera. Originally designed as a commodity level camera for home games consoles, it was available directly to consumers. This means that Kinect cameras are widely and cheaply available to use.

However, they provide high quality information considering their price. This is provided to the system in the form of an ordinary RGB image, augmented with a fourth parameter indicating the perceived depth from the camera at that pixel. Using the available depth information, it is also possible to compute the normal to the surface at each pixel of the image. In some cases, depth information may be missing at certain pixels, producing so-called `holes' in the depth image. Errors such as these are not difficult to overcome but must be handled.


The sensor is capable of producing an RGB-D image to the system in 640x480 resolution at up to 30Hz. This is important, as it determines the amount of information that can be added to the system at each update. Additionally, it places a hard limit on the update rate of the system, which can never exceed the sensor rate.

\section{Rigid Surface Reconstruction}
\subsection{KinectFusion}

KinectFusion provides one such method for determining correspondences between point clouds. It makes one very important assumption: that the surface it is reconstructing is rigid, and will not be deformed while it is being observed by the system. While this is a very restricted approach to the problem, this algorithm laid the groundwork for later non-rigid systems such as DynamicFusion.

\subsubsection{Iterative Closest Point algorithm}
This is a widely used algorithm in this field which has been optimised and adapted for many purposes. Its purpose is simply to take two point clouds and return the transformation which most closely aligns the two clouds.
Psuedo-code for the ICP algorithm is shown below:

\begin{lstlisting}[language=Python]
def ICP(P, Q):
    transform = identity
    while not aligned(P, Q):
        correspondence = []
        for p in P:
            p_t = apply(transform, p)
            q = closest point to p_t in Q
            correspondence += (p, q)
        transform = least_squares(correspondence)
    return transform
\end{lstlisting}

\subsubsection{Principles of operation}

Initially, a pre-processing phase occurs, where depth and normal maps are constructed from the input data. The system then enters a continuously updating loop with three stages.

\begin{enumerate}
\item \textbf{Sensor pose estimation}: KinectFusion uses the ICP algorithm to align its prediction of the surface with the actual measurements of the surface it receives. The transform that is returned is rigid, which functions perfectly for this algorithm, but requires adaptation for a non-rigid case. 

\item \textbf{Update surface reconstruction}: Here, the system receives input from the sensor, which it uses to create a truncated signed distance function (TSDF) for that live frame. Together with the TSDFs from previous frames, a composite TSDF is created to represent the system's model of the scene. This composite TSDF is discretised into a 2-dimensional array and stored in global GPU memory.

\item \textbf{Surface prediction}: At this stage KinectFusion will produce its own depth map by ray-casting against its own model of the scene. This produces a prediction of what it expects the scene to look like.
\end{enumerate}


\section{Non-rigid Surface Reconstruction}
\subsection{DynamicFusion}

DynamicFusion was the first system to achieve dense, dynamic scene reconstruction in real-time. Both VolumeDeform and KillingFusion were heavily inspired by or based on DynamicFusion.

\subsubsection{Warp field}

The warp field is essentially a set of deformation nodes, each of which is denoted:

$$\mathcal{N}^t_{\textbf{warp}}=\{ \textbf{dg}_v, \textbf{dg}_w, \textbf{dg}_{se3} \}$$

Where the parameters refer to:

\begin{itemize}
\item $\textbf{dg}_v$: The position vector of the deformation node

\item $\textbf{dg}_w$: The radius of influence of the node, which determines the strength of the application of the transformation to the node's neighbours

\item $\textbf{dg}_{se3}$: The transformation of the deformation node, represented using dual quaternions
\end{itemize}


\subsubsection{Dual quaternions}

3


\subsubsection{Dual quaternion blending}

KD-Tree

$$\mathcal{W}(x_c) = SE3(\textbf{DQB}(x_c))$$

$$\textbf{DQB}(x_c) = \frac{\sum_{k \in N(x_c)} \textbf{w}_k(x_c)\hat{\textbf{q}}_{kc}}{\Vert \sum_{k \in N(x_c)} \textbf{w}_k(x_c)\hat{\textbf{q}}_{kc} \Vert}$$

\subsubsection{Warp function}

6

\subsubsection{Surface fusion}

7

\noindent\textbf{Principles of operation}


There are three key steps that occur in each iteration of the DynamicFusion algorithm, and they each take place whenever a new live frame is produced:

\begin{enumerate}
\item \textbf{Warp field estimation}: The warp field is estimated by choosing parameters such that the error function below is minimised:

$$E(\mathcal{W}_t, \mathcal{V}, D_t, \mathcal{E}) = \textbf{Data}(\mathcal{W}_t, \mathcal{V}_t, D_t) + \lambda\textbf{Reg}(\mathcal{W}_t, \mathcal{E})$$

This function consists of two terms:
\begin{itemize}
\item $\textbf{Data}(\mathcal{W}_t, \mathcal{V}_t, D_t) = \sum\limits_{u \in \Omega} \psi_{data}(\hat{\textbf{n}}_u^\top (\hat{\textbf{v}}_u - \textbf{vl}_u))$

This term represents the ICP cost between the model and the live frame. This step is similar to KinectFusion, although it uses a non-rigid version of ICP.

\item $\textbf{Reg}(\mathcal{W}_t, \mathcal{E}) = \sum\limits_{i=0}^n\sum\limits_{j \in \mathcal{E}(i)}\alpha_{ij}\psi_{reg}(\textbf{T}_{ic}\textbf{dg}^j_v - \textbf{T}_{jc}\textbf{dg}^j_v)$

This term which regularises the warp field. This ensures that motion is as smooth as possible, and that deformation is as rigid as possible.
\end{itemize}

Additionally, the $\lambda$ parameter can be adjusted to determine the relative importance of each term to the error function as a whole.

$E$ is minimised using Gauss-Newton non-linear optimisation.

\item \textbf{Surface fusion}: Using the estimated warp field, both surfaces can be warped back into the canonical model. This means they occupy the same space relative to each other, so can be directly compared.

\item \textbf{Updating the canonical model}: Now that the live frame has been warped into the canonical model, points can be added or removed from the canonical model as appropriate.
\end{enumerate}


\subsection{VolumeDeform}

VolumeDeform is different to the other examined techniques in that it doesn't rely solely on depth information. It also uses the RGB data probided by the sensor to extract additional information about the live frame.\\


\noindent\textbf{Principles of operation}

VolumeDeform operates similarly to the other methods outlined, in that it involves a loop that repeatedly acquires new knowledge and integrates it with an existing model.

\begin{enumerate}
\item \textbf{Mesh extraction}: The signed distance field representing the system's model of the scene is used to generate a deformed 3D mesh using Marching Cubes
\item \textbf{Correspondence detection}: The mesh is rendered to produce a depth map which can be used to generate correspondences.

Additionally, SIFT features are detected in the live frame and stored by the system. Since SIFT features are robust to changes in transformation, they can be recognised again in later frames. This provides extra information to inform the process of correspondence detection.

\item \textbf{Deformation optimisation}: The deformation field is optimised so that it is consistent with the observed values of colour and depth.
\end{enumerate}


\section{SDF-2-SDF}

SDF-2-SDF is a technique for static reconstruction.

precursor to KillingFusion, and designed in a very similar way

operates more explicitly - no notion of point clouds, so data of depth image is abstracted into a SDF. 

creates a canonical SDF by accumulating data from each frame

\subsection{Signed distance field}

A signed distance field (SDF) is a mapping $\phi : \mathbb{R}^3 \rightarrow  \mathbb{R}$ which yields the distance from the surface of an object at a particular point in space. 
Points inside the object correspond to negative values; points outside the object correspond to positive values and points on the surface are mapped to zero.

One of the important properties of a signed distance field is that the magnitude of the gradient of the SDF is equal to one at all points. This is called the level set property and is useful as a regularizer.

additionally, the gradient of the sdf is equivalent to the normal of the surface at the given point.

\subsection{Voxel grid}

the volume to be reconstructed is divided up into a regular grid of voxels. resolution 5-10 millimetres - mention sensor error of kinect camera

a gradient descent scheme is used to iteratively align the current SDF to the canonical sdf. this process is repeated until the magnitude of the maximum update is less than a certain threshold (0.1 millimetres)

$$\Psi_{n+1} = \Psi_n - \alpha E(\Psi)$$




\subsection{energy functions}

simplest:

$$ E_{geom} = \frac{1}{2} \sum\limits_{\textrm{all voxels}} (\phi_n(\Psi) - \phi_{global})^2  $$


additionally, the normals can be constrained to be approximately 1. Originally the dot product of the two relevant normals was used

$$ E_{norm} = \sum\limits_{\textrm{all voxels}}(1 - \nabla \phi_n(\Psi) \cdot \nabla\phi_{global}) $$

These two error functions can be combined proportionally using a scaling factor $\alpha_{norm}$. This factor can take on values in the range $[0..1]$, but is usually set to zero to improve performance.

$$E = E_{geom} + \alpha_{norm}E_{norm}$$

$$ E'_{geom} = (\phi_n(\Psi) - \phi_{global}) \nabla \phi_n(\Psi)$$

\subsection{Weighted average accumulation}

at the end of each update to $\Psi$, the accurately posed SDF must be integrated into the canonical SDF. This is done using a weighted average of all previous SDFs

\begin{align*}
\Phi_{n+1}(\textbf{V}) &= \frac{W_n(\textbf{V})\Phi_n(\textbf{V}) + \omega_{n+1}(\textbf{V})\phi_{n+1}(\textbf{V})}{W_{n}(\textbf{V}) + \omega_{n+1}(\textbf{V}}\\
W_{n+1}(\textbf{V}) &= W_n(\textbf{V}) + \omega_n(\textbf{V}) \\
\end{align*}

Here, the voxel weighting is defined as:

\[
    \omega(\textbf{V}) = \left\{\begin{array}{lr}
    1, & \text{for } \phi_{true}(\textbf{V}) > - \eta\\
    0, & \text{otherwise}\\
    \end{array}\right\}
\]

$\eta$ is a hyper-parameter that determines the expected thickness of the object 

In practice, the SDFs produced can be sampled at each voxel throughout the volume and a simple test can be performed to determine whether or not to increment $\Phi_n(\textbf{V})$ and $W_n(\textbf{V})$

\section{KillingFusion}

KillingFusion operates very similarly to SDF-2-SDF, but introduces two other terms to the error function to enable it to estimate a non-rigid transformation.

\begin{align*}
E_{non-rigid}(\Psi) &= E_{data}(\Psi) + \omega_kE_{Killing}(\Psi) + \omega_sE_{level-set}(\Psi)\\
E'_{data}(\Psi) &= (\phi_n(\Psi) - \phi_{global}) \nabla_{\phi_n}(\Psi)\\
E'_{Killing}(\Psi) &= 2H_{uvw}(vec(J^{\top}_{\Psi}) vec\big(J_{\Psi})\big)\begin{pmatrix}1\\\gamma \end{pmatrix}\\
E'_{level-set}(\Psi) &= \frac{|\nabla_{\phi_n}(\Psi)| - 1}{|\nabla_{\phi_n}(\Psi)|_\epsilon}H_{\phi_n}(\Psi)\nabla_{\phi_n}(\Psi)
\end{align*}

This error function imposes three requirements on a potential solution:
\begin{itemize}
\item \textbf{Data term}: Similar to DynamicFusion, contains a component for non-rigid transformation.
\item \textbf{Killing condition}: This term requires the flow field to be a Killing vector field. If it satisifies this condition, then it will produce isometric motion Since a perfect isometric motion would impose only rigid transformations, this term only needs to be minimised so that the motion is approximately rigid. Additionally, the requirement has been weakened to make it computationally more efficient.
\item \textbf{Level set property}: The minimisation of this term ensures that the gradient of the SDF function remains approximately 1
\end{itemize}

The additional parameters $\omega_k$ and $\omega_s$ are used to adjust the relative importance of the three terms when calculating $E_{non-rigid}$.

This error function is also much more easily parallelised, leading to greatly increased performance.\\


\subsection{Killing vector field}

\subsection{Deformation field}
What it is theoretically:
Mapping psi: R3 -> R3 which describes a translation of that voxel which deforms 3D space.

How i did it:
3d voxel grid of vectors

How its estimated:
Gradient descent scheme

Define error function!

Rigid deformation done first:
Difference of distances in SDFs, phin to phiglobal

Non rigid deformation done next
Two more componenets added
Killing energy:
Preserves the smoothness of deformation field

Level set energy:
Keeps the magnitude of the gradient of the SDF approximately 1

Then we need to find the gradient of the error functions for the gradient descent schema

Rigid:

Non-rigid:

Optimisations
Designed to be done in parallel: i did multi-threaded on CPU, could also do GPU but not sure about time :/


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Contribution}

\section{CPU version}

The code for this part of the project can be found at:

\begin{center}
\verb|github.com/millie-crowther/fusion|
\end{center}

\subsection{Multi-threading}

The design of the KillingFusion system is very well suited to parallelism, since the update of each voxel only depends on the evaluation of the error function at that voxel. 
Of course, given the size of the voxel grid involved, it is best to use the massive parallelism of a GPU to maximise performance.
However, it is still possible to parallelise the system on the CPU and achieve a moderate increase in performance. 

make a diagram for running time with different sized thread pools

\section{GPU version}

The code for this part of the project can be found at:

\begin{center}
\verb|github.com/millie-crowther/gpu_fusion|
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimental Results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}


%% bibliography
\bibliographystyle{apa}


\end{document}
